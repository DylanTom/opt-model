{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e61288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "from curl_cffi import requests\n",
    "session = requests.Session(impersonate=\"chrome\")\n",
    "\n",
    "import cvxpy as cp\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7866fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = [15, 30, 45, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe06f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = os.path.join(os.path.dirname(os.getcwd()), '.env')\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "def get_chatgpt_tickers(n):\n",
    "    client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    message = f\"Using a range of investing principles taken from leading funds, create a theoretical fund comprising of at least {n} stocks (mention their tickers) from the S&P500 with the goal to outperform the S&P500 index. Return as a list that can be parsed in python.\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[{\"role\": \"user\", \"content\": message}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Loop through each value and make 10 requests\n",
    "with open(os.path.join(\"..\", \"data/chatgpt_tickers.txt\"), \"w\") as f:\n",
    "    for n in choices:\n",
    "        for i in range(10):\n",
    "            result = get_chatgpt_tickers(n)\n",
    "            f.write(f\"\\n=== N={n} | Request {i+1} ===\\n\")\n",
    "            f.write(result + \"\\n\")\n",
    "            print(f\"[✔] Saved N={n}, Request {i+1}\")\n",
    "            time.sleep(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd162250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def extract_all_ticker_lists(filepath):\n",
    "    request_blocks = {}\n",
    "    current_key = None\n",
    "    inside_list = False\n",
    "    buffer = []\n",
    "\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Detect a request header line\n",
    "            if line.startswith(\"===\"):\n",
    "                current_key = line.strip(\"= \").strip()\n",
    "                inside_list = False\n",
    "                buffer = []\n",
    "                continue\n",
    "\n",
    "            # Detect any opening of a list\n",
    "            if \"[\" in line and not inside_list and current_key:\n",
    "                inside_list = True\n",
    "                buffer = [line[line.find(\"[\"):]]  # keep only from the first `[`\n",
    "                continue\n",
    "\n",
    "            # Collect lines until the list is closed\n",
    "            if inside_list:\n",
    "                buffer.append(line)\n",
    "                if \"]\" in line:\n",
    "                    inside_list = False\n",
    "                    try:\n",
    "                        full_text = \"\\n\".join(buffer)\n",
    "                        tickers = ast.literal_eval(full_text)\n",
    "                        request_blocks[current_key] = tickers\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ Failed to parse tickers for {current_key}: {e}\")\n",
    "                        print(\"Block was:\\n\", full_text)\n",
    "                    buffer = []\n",
    "\n",
    "    return request_blocks\n",
    "\n",
    "data = extract_all_ticker_lists(os.path.join(\"..\", \"data/chatgpt_tickers.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f57cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    {\n",
    "        \"request\": k,\n",
    "        \"n\": int(k.split(\"|\")[0].split(\"=\")[1].strip()),\n",
    "        \"tickers\": v,\n",
    "        \"count\": len(v)\n",
    "    }\n",
    "    for k, v in data.items()\n",
    "])\n",
    "\n",
    "df_exploded = df.explode(\"tickers\").rename(columns={\"tickers\": \"ticker\"})\n",
    "\n",
    "# Preview\n",
    "print(df.head())\n",
    "print(df_exploded[\"ticker\"].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221114e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "for i in choices:\n",
    "    df = df_exploded[df_exploded[\"n\"] == i]\n",
    "    ticker_counts = df['ticker'].value_counts()\n",
    "\n",
    "    top_n = i\n",
    "    top_tickers = ticker_counts.head(top_n)\n",
    "    other_tickers = ticker_counts.iloc[top_n:]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    plt.bar(top_tickers.index, top_tickers.values, color='green', label=f\"Top {i} stocks\")\n",
    "    plt.bar(other_tickers.index, other_tickers.values, color='red', hatch='//', label='Other stocks')\n",
    "\n",
    "    plt.xticks(rotation=90, ha=\"right\", fontsize=5)\n",
    "    plt.xlabel(\"Stock Tickers\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Universe Selected by GPT-4 for the Top {i} Stocks\")\n",
    "\n",
    "    plt.savefig(os.path.join('..',f\"figs/top_{i}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e22fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chatgpt_insight(ticker):\n",
    "    \"\"\"Generate a prompt and request from OpenAI about the sentiment of a given company.\"\"\"\n",
    "\n",
    "    client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    prompt = (\n",
    "        f\"Given the following stock {ticker}, \"\n",
    "        \"please write a report on the current state of the company. \"\n",
    "        \"Please be clear on when you collected the most recent data. \"\n",
    "        \"What are the key insights of each company? \"\n",
    "        \"Please highlight the positives and negatives in an objective way for fair sentiment analysis.\"\n",
    "        \"Include any information from their financial reports or news.\"\n",
    "    )\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e29830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_top_stocks(df, top_values=[15, 30, 45, 60], output_dir=\"sentiment\"):\n",
    "    \"\"\"Process top X stocks for each choice and save insights to text files.\"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(f\"../data/{output_dir}\", exist_ok=True)\n",
    "\n",
    "    # Collect all top stocks across the specified top values\n",
    "    unique_stocks = set()\n",
    "    for top_x in top_values:\n",
    "        # Get top X stocks for each choice and add to the set\n",
    "        top_stocks = df.groupby(\"request\")[\"ticker\"].value_counts().groupby(level=0).nlargest(top_x).reset_index(level=0, drop=True)\n",
    "        unique_stocks.update(top_stocks.index.get_level_values(1))\n",
    "\n",
    "    for ticker in tqdm.tqdm(unique_stocks, desc=\"Generating Insights\"):\n",
    "        file_path = os.path.join(f\"../data/{output_dir}\", f\"{ticker}.txt\")\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            insight = generate_chatgpt_insight(ticker)\n",
    "        except Exception as e:\n",
    "            insight = f\"Error generating insight: {str(e)}\"\n",
    "        \n",
    "        # Save the insight to the file\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(f\"Ticker: {ticker}\\n\")\n",
    "            file.write(f\"Insight:\\n{insight}\\n\")\n",
    "    \n",
    "    print(f\"Insights saved in the '{output_dir}' directory.\")\n",
    "\n",
    "process_top_stocks(df_exploded, top_values=[15, 30, 45, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c958337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stock_data(ticker, output_dir=\"market\", retries=5):\n",
    "    file_path = os.path.join(f\"../data/{output_dir}\", f\"{ticker}.csv\")\n",
    "    if os.path.exists(file_path):\n",
    "        return f\"{ticker} - Already downloaded.\"\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            data = yf.download(ticker, start=\"2010-01-01\", end=\"2025-01-01\")\n",
    "            if data.empty:\n",
    "                return f\"{ticker} - No data available.\"\n",
    "            \n",
    "            # Save the data to file\n",
    "            data.to_csv(file_path)\n",
    "            return f\"{ticker} - Downloaded successfully.\"\n",
    "        \n",
    "        except yf.YFRateLimitError:\n",
    "            attempt += 1\n",
    "            wait_time = 2 ** attempt  # Exponential backoff\n",
    "            print(f\"{ticker} - Rate limited. Retrying in {wait_time}s...\")\n",
    "            time.sleep(wait_time)\n",
    "        except Exception as e:\n",
    "            return f\"{ticker} - Error: {str(e)}\"\n",
    "    \n",
    "    return f\"{ticker} - Failed after {retries} retries.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26220bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_top_stocks_data(df, top_values=[15, 30, 45, 60], output_dir=\"market\"):\n",
    "    \"\"\"Process top X stocks for each choice and save insights to text files.\"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(f\"../data/{output_dir}\", exist_ok=True)\n",
    "\n",
    "    unique_stocks = set()\n",
    "    for top_x in top_values:\n",
    "        top_stocks = df.groupby(\"request\")[\"ticker\"].value_counts().groupby(level=0).nlargest(top_x).reset_index(level=0, drop=True)\n",
    "        unique_stocks.update(top_stocks.index.get_level_values(1))\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = {executor.submit(download_stock_data, ticker, output_dir): ticker for ticker in unique_stocks}\n",
    "        \n",
    "        for future in tqdm.tqdm(as_completed(futures), total=len(futures), desc=\"Downloading data\"):\n",
    "            ticker = futures[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                print(result)\n",
    "            except Exception as e:\n",
    "                print(f\"{ticker} - Unexpected Error: {str(e)}\")\n",
    "\n",
    "process_top_stocks_data(df_exploded, top_values=[15, 30, 45, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ecd16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chatgpt_allocation(stocks):\n",
    "    \"\"\"Generates a theoretical model portfolio insight for the given stocks using ChatGPT.\"\"\"\n",
    "\n",
    "    client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Assume you're designing a theoretical model portfolio from these S&P500 stocks: {', '.join(stocks)}. \"\n",
    "        \"Provide an example of how you might distribute the weightage of these stocks (normalized, i.e., weights should add up to 1.00) in the portfolio \"\n",
    "        \"to potentially outperform the S&P500 index. Also mention the underlying strategy or logic which you used to assign these weights. \"\n",
    "        \"Please provide any additional rationale when deciding the allocation.\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chatgpt_responses(df, top_values=[15, 30, 45, 60], output_dir=\"chatgpt_responses\"):\n",
    "    os.makedirs(f\"../data/{output_dir}\", exist_ok=True)\n",
    "    \n",
    "    # Collect top X stocks for each specified value\n",
    "    for n in top_values:\n",
    "        df_filtered = df[df['request'].str.contains(f\"N={n}\")]\n",
    "        top_n_stocks = df_filtered['ticker'].value_counts().head(n).index.tolist()\n",
    "\n",
    "        response = generate_chatgpt_allocation(list(top_n_stocks))\n",
    "\n",
    "        # Save the response to a text file\n",
    "        file_path = os.path.join(f\"../data/{output_dir}\", f\"top_{n}_stocks_portfolio.txt\")\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(f\"Top {n} Stocks: {', '.join(top_n_stocks)}\\n\\n\")\n",
    "            file.write(response)\n",
    "        \n",
    "    print(f\"ChatGPT responses saved in the '{output_dir}' directory.\")\n",
    "save_chatgpt_responses(df_exploded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70c7d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c967f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_directory = \"../data/market\" \n",
    "\n",
    "# Define the n values for the four groups\n",
    "n_values = [15, 30, 45, 60]\n",
    "\n",
    "def plot_top_n_stocks(df, csv_directory, n_values):\n",
    "    for n in n_values:\n",
    "        os.makedirs(f\"../figs/stock_price\", exist_ok=True)\n",
    "        df_filtered = df[df['request'].str.contains(f\"N={n}\")]\n",
    "        \n",
    "        # Calculate the top N stocks for this group\n",
    "        top_n_stocks = df_filtered['ticker'].value_counts().head(n).index.tolist()\n",
    "\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        plotted_stocks = 0  \n",
    "\n",
    "        for ticker in top_n_stocks:\n",
    "            csv_path = os.path.join(csv_directory, f\"{ticker}.csv\")\n",
    "            if os.path.exists(csv_path):\n",
    "                stock_data = pd.read_csv(\n",
    "                    csv_path, \n",
    "                    skiprows=2, \n",
    "                    names=[\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"],\n",
    "                    dtype={\n",
    "                        \"Date\": str, \n",
    "                        \"Close\": float, \n",
    "                        \"High\": float, \n",
    "                        \"Low\": float, \n",
    "                        \"Open\": float, \n",
    "                        \"Volume\": float\n",
    "                    },\n",
    "                    na_values=[\"\", \"NaN\", \"N/A\", \"null\"]\n",
    "                )\n",
    "\n",
    "                # Dropping rows with invalid dates or NaN values in essential columns\n",
    "                stock_data = stock_data.dropna(subset=[\"Date\", \"Close\"])\n",
    "\n",
    "                # Converting Date column to datetime \n",
    "                stock_data[\"Date\"] = pd.to_datetime(stock_data[\"Date\"], errors=\"coerce\")\n",
    "                stock_data = stock_data.dropna(subset=[\"Date\"])\n",
    "\n",
    "                # Setting Date as index\n",
    "                stock_data.set_index(\"Date\", inplace=True)\n",
    "                if 'Close' in stock_data.columns:\n",
    "                    plt.plot(stock_data['Close'], label=ticker)\n",
    "                    plotted_stocks += 1\n",
    "     \n",
    "        if plotted_stocks > 0:\n",
    "            plt.title(f\"Top {n} Stocks - Price vs Time\")\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(\"Stock Price (USD)\")\n",
    "            if n==60 or n==45:\n",
    "                plt.legend(loc=\"upper left\", fontsize=8, ncol=2)\n",
    "            else:\n",
    "                plt.legend(loc=\"upper left\", fontsize=8)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"../figs/stock_price/top_{n}.png\")\n",
    "        else:\n",
    "            print(f\"No valid data for Top {n} Stocks.\\n\")\n",
    "\n",
    "# Call the function\n",
    "plot_top_n_stocks(df_exploded, csv_directory, n_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71868726",
   "metadata": {},
   "source": [
    "### Pure MVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d152c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns_df(n):\n",
    "    df_filtered = df_exploded[df_exploded['request'].str.contains(f\"N={n}\")]\n",
    "    \n",
    "    top_n_stocks = df_filtered['ticker'].value_counts().head(n).index.tolist()\n",
    "\n",
    "    stock_data = {}\n",
    "\n",
    "    for ticker in top_n_stocks:\n",
    "        file_name =  f\"../data/market/{ticker}.csv\"\n",
    "        try:\n",
    "            df_raw = pd.read_csv(file_name, skiprows=[1, 2])\n",
    "            \n",
    "            if ticker == \"UNH\":\n",
    "                df_raw = df_raw.iloc[:, [0, 2, 5, 8, 11, 14]]\n",
    "            else:\n",
    "                df_raw = df_raw.iloc[:, :6]\n",
    "        \n",
    "            df_raw.columns = [\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n",
    "            df_raw.dropna(how=\"all\", inplace=True) \n",
    "\n",
    "            # Converting the Date column to datetime format\n",
    "            df_raw[\"Date\"] = pd.to_datetime(df_raw[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "            # Converting the Close column to float \n",
    "            df_raw[\"Close\"] = pd.to_numeric(df_raw[\"Close\"], errors=\"coerce\")\n",
    "\n",
    "            df_clean = df_raw.set_index(\"Date\")\n",
    "            df_clean[\"Returns\"] = df_clean[\"Close\"].pct_change(fill_method=None) * 100\n",
    "            stock_data[ticker] = df_clean[\"Returns\"]\n",
    "        except Exception as e:\n",
    "            print(ticker, e)\n",
    "\n",
    "    returns_df = pd.DataFrame()\n",
    "\n",
    "    for stock, df in stock_data.items():\n",
    "        returns_df[stock] = df\n",
    "\n",
    "    return returns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d9c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_15_returns = calculate_returns_df(15)\n",
    "top_30_returns = calculate_returns_df(30)\n",
    "top_45_returns = calculate_returns_df(45)\n",
    "top_60_returns = calculate_returns_df(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5722c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_60_returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608a8e8",
   "metadata": {},
   "source": [
    "### 1. Equally Weighted Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813acb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../figs/backtests/equal', exist_ok=True)\n",
    "\n",
    "def compute_metrics(returns, n):\n",
    "\t# Ensure returns are in decimal form and handle NaN values\n",
    "    returns = returns / 100  \n",
    "    returns.fillna(0, inplace=True)  \n",
    "\n",
    "    # Calculate the equally weighted portfolio returns\n",
    "    equal_weights = 1 / returns.shape[1]\n",
    "    portfolio_returns = (returns * equal_weights).mean(axis=1)\n",
    "\n",
    "    # Calculate cumulative returns\n",
    "    cumulative_returns = (1 + portfolio_returns).cumprod()\n",
    "\n",
    "    # Plot the cumulative returns\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(cumulative_returns, label='Equally Weighted Portfolio')\n",
    "    plt.title(f\"Equally Weighted Portfolio Backtest n={n}\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Returns')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.savefig(f'../figs/backtests/equal/top_{n}.png')\n",
    "\n",
    "    # Performance Metrics\n",
    "    total_return = cumulative_returns.iloc[-1] - 1\n",
    "    annualized_return = (cumulative_returns.iloc[-1]) ** (252 / len(cumulative_returns)) - 1\n",
    "    annualized_volatility = portfolio_returns.std() * np.sqrt(252)\n",
    "    sharpe_ratio = annualized_return / annualized_volatility\n",
    "\n",
    "    # Display metrics\n",
    "    print(f'Total Return: {total_return:.2%}')\n",
    "    print(f'Annualized Return: {annualized_return:.2%}')\n",
    "    print(f'Annualized Volatility: {annualized_volatility:.2%}')\n",
    "    print(f'Sharpe Ratio: {sharpe_ratio:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(top_15_returns, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7686ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(top_30_returns, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610a5712",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(top_45_returns, 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e6695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(top_60_returns, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b038f60a",
   "metadata": {},
   "source": [
    "# GPT Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c6f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../figs/backtests/gpt', exist_ok=True)\n",
    "\n",
    "def compute_gpt_returns(returns, n, custom_weights):\n",
    "    # Ensure returns are in decimal form and handle NaN values\n",
    "    returns = returns / 100  \n",
    "    returns.fillna(0, inplace=True)  \n",
    "\n",
    "    dropped_columns = set(custom_weights.keys()) - set(returns.columns)\n",
    "    if dropped_columns:\n",
    "        print(f\"Missing columns in returns data: {dropped_columns}\")\n",
    "\n",
    "    aligned_weights = {ticker: weight for ticker, weight in custom_weights.items() if ticker in returns.columns}\n",
    "\n",
    "    portfolio_returns = (returns[list(aligned_weights.keys())] * list(aligned_weights.values())).sum(axis=1)\n",
    "\n",
    "    cumulative_returns = (1 + portfolio_returns).cumprod()\n",
    "\n",
    "    # Plot the cumulative returns\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(cumulative_returns, label='Custom Weighted Portfolio')\n",
    "    plt.title(f'GPT Weighted Portfolio Backtest n={n}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Returns')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.savefig(f\"../figs/backtests/gpt/top_{n}.png\")\n",
    "\n",
    "    # Performance Metrics\n",
    "    total_return = cumulative_returns.iloc[-1] - 1\n",
    "    annualized_return = (cumulative_returns.iloc[-1]) ** (252 / len(cumulative_returns)) - 1\n",
    "    annualized_volatility = portfolio_returns.std() * np.sqrt(252)\n",
    "    sharpe_ratio = annualized_return / annualized_volatility\n",
    "\n",
    "    # Display metrics\n",
    "    print(f'Total Return: {total_return:.2%}')\n",
    "    print(f'Annualized Return: {annualized_return:.2%}')\n",
    "    print(f'Annualized Volatility: {annualized_volatility:.2%}')\n",
    "    print(f'Sharpe Ratio: {sharpe_ratio:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57da924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_15_weights = {\n",
    "        'MSFT': 0.130,\n",
    "        'AAPL': 0.120,\n",
    "        'NVDA': 0.105,\n",
    "        'AMZN': 0.085,\n",
    "        'GOOGL': 0.080,\n",
    "        'META': 0.060,\n",
    "        'AVGO': 0.055,\n",
    "        'LLY': 0.075,\n",
    "        'UNH': 0.050,\n",
    "        'JPM': 0.040,\n",
    "        'MA': 0.040,\n",
    "        'V': 0.040,\n",
    "        'COST': 0.045,\n",
    "        'LIN': 0.030,\n",
    "        'HD': 0.030\n",
    "    }\n",
    "\n",
    "compute_gpt_returns(top_15_returns, 15, custom_15_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971a3984",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_30_weights = {\n",
    "    'AAPL': 0.06,\n",
    "    'MSFT': 0.06,\n",
    "    'GOOGL': 0.06,\n",
    "    'NVDA': 0.07,\n",
    "    'AMZN': 0.06,\n",
    "    'META': 0.06,\n",
    "    'AVGO': 0.05,\n",
    "    'UNH': 0.03,\n",
    "    'LLY': 0.05,\n",
    "    'COST': 0.04,\n",
    "    'HD': 0.03,\n",
    "    'NKE': 0.02,\n",
    "    'LIN': 0.02,\n",
    "    'ADBE': 0.04,\n",
    "    'V': 0.03,\n",
    "    'MA': 0.03,\n",
    "    'MRK': 0.02,\n",
    "    'TMO': 0.03,\n",
    "    'JNJ': 0.03,\n",
    "    'WMT': 0.04,\n",
    "    'PGR': 0.02,\n",
    "    'CRM': 0.03,\n",
    "    'TSLA': 0.04,\n",
    "    'SCHW': 0.02,\n",
    "    'PEP': 0.03,\n",
    "    'ORCL': 0.03,\n",
    "    'PG': 0.03,\n",
    "    'JPM': 0.03,\n",
    "    'SPGI': 0.02\n",
    "}\n",
    "\n",
    "compute_gpt_returns(top_30_returns, 30, custom_30_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a06ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_45_weights = {\n",
    "\t'AAPL': 0.07,\n",
    "    'MSFT': 0.08,\n",
    "    'GOOGL': 0.065,\n",
    "    'AMZN': 0.06,\n",
    "    'NVDA': 0.07,\n",
    "    'META': 0.045,\n",
    "    'AVGO': 0.035,\n",
    "    'AMD': 0.02,\n",
    "    'ADBE': 0.015,\n",
    "    'ORCL': 0.01,\n",
    "    'CRM': 0.01,\n",
    "    'NFLX': 0.015,\n",
    "    'TSLA': 0.02,\n",
    "    'UNH': 0.025,\n",
    "    'LLY': 0.03,\n",
    "    'JNJ': 0.02,\n",
    "    'MRK': 0.015,\n",
    "    'REGN': 0.01,\n",
    "    'VRTX': 0.01,\n",
    "    'TMO': 0.01,\n",
    "    'ABBV': 0.01,\n",
    "    'CAT': 0.015,\n",
    "    'DE': 0.01,\n",
    "    'HON': 0.01,\n",
    "    'LMT': 0.01,\n",
    "    'LIN': 0.01,\n",
    "    'MS': 0.01,\n",
    "    'JPM': 0.015,\n",
    "    'BAC': 0.005,\n",
    "    'GS': 0.005,\n",
    "    'SCHW': 0.005,\n",
    "    'MA': 0.025,\n",
    "    'V': 0.02,\n",
    "    'PEP': 0.015,\n",
    "    'KO': 0.01,\n",
    "    'PG': 0.015,\n",
    "    'WMT': 0.015,\n",
    "    'COST': 0.02,\n",
    "    'HD': 0.01,\n",
    "    'TJX': 0.01,\n",
    "    'CMG': 0.01,\n",
    "    'SBUX': 0.01,\n",
    "    'NKE': 0.01,\n",
    "    'NEE': 0.01\n",
    "}\n",
    "\n",
    "compute_gpt_returns(top_45_returns, 45, custom_45_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e36457",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_60_weights = {\n",
    "\t'AAPL': 0.05, 'MCD': 0.02, 'UNP': 0.015, 'PLD': 0.01,\n",
    "    'MSFT': 0.05, 'TMO': 0.015, 'LMT': 0.01, 'SLB': 0.01,\n",
    "    'CAT': 0.015, 'ADBE': 0.02, 'HON': 0.015, 'EOG': 0.01,\n",
    "    'MS': 0.015, 'COST': 0.01, 'PG': 0.02, 'ABBV': 0.02,\n",
    "    'MCD': 0.02, 'UNH': 0.02, 'BLK': 0.015, 'BAC': 0.01,\n",
    "    'TMO': 0.015, 'AMD': 0.015, 'PEP': 0.015, 'ACN': 0.02,\n",
    "    'ADBE': 0.02, 'JPM': 0.02, 'CRM': 0.015, 'SBUX': 0.01,\n",
    "    'COST': 0.01, 'AVGO': 0.02, 'NFLX': 0.015, 'CVX': 0.01,\n",
    "    'UNH': 0.02, 'NVDA': 0.03, 'INTU': 0.015, 'ABT': 0.01,\n",
    "    'AMD': 0.015, 'LLY': 0.03, 'HD': 0.015, 'CL': 0.005,\n",
    "    'JPM': 0.02, 'GOOGL': 0.03, 'DE': 0.01, 'RTX': 0.01,\n",
    "    'AVGO': 0.02, 'AMZN': 0.02, 'LOW': 0.01, 'CSCO': 0.01,\n",
    "    'NVDA': 0.03, 'META': 0.02, 'VRTX': 0.01, 'TSLA': 0.02,\n",
    "    'LLY': 0.03, 'UNP': 0.015, 'GS': 0.01, 'UPS': 0.01,\n",
    "    'GOOGL': 0.03, 'LMT': 0.01, 'SCHW': 0.01, 'MA': 0.015,\n",
    "    'AMZN': 0.02, 'HON': 0.015, 'REGN': 0.01, 'NEE': 0.015,\n",
    "    'META': 0.02, 'PG': 0.02, 'JNJ': 0.02, 'LIN': 0.01,\n",
    "    'ORCL': 0.015, 'NKE': 0.01, 'XOM': 0.01, 'WMT': 0.02,\n",
    "    'REGN': 0.01, 'V': 0.015, 'MRK': 0.01, 'KO': 0.015\n",
    "}\n",
    "\n",
    "compute_gpt_returns(top_60_returns, 60, custom_60_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b1ce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0.08\n",
    "\n",
    "w = cp.Variable(15)\n",
    "objective = cp.Minimize(cp.quad_form(w, sigma))\n",
    "constraints = [\n",
    "    cp.sum(w) == 1,           \n",
    "    w @ mu >= r,\n",
    "\tw >= 0,      \n",
    "]\n",
    "\n",
    "problem = cp.Problem(objective, constraints)\n",
    "\n",
    "try:\n",
    "    problem.solve(solver = cp.CLARABEL)\n",
    "    print (\"Optimal portfolio\")\n",
    "    print (\"----------------------\")\n",
    "    for s in range(len(mu)):\n",
    "        print (\" Investment in {} : {}% of the portfolio\".format(s + 1,round(100*w.value[s],2)))\n",
    "    print (\"----------------------\")\n",
    "    print (f\"Expected return = {(w.value @ mu)}%\")\n",
    "    print (f\"Expected risk = {np.sqrt(w.value.T @ sigma @ w.value)}%\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef3212b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
